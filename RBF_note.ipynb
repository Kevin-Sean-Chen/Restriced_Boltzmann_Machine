{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzman Machine (RBM)\n",
    "\n",
    "The classic reccurent neural network RBM starts with the energy function (Hamiltonian) according to the connection and activity in visible and hidden layer of neurons:\n",
    "\n",
    "$E(s,\\lambda)=-\\Sigma a_iv_i - \\Sigma b_jh_j - \\Sigma v_i w_{ij} h_j$\n",
    "\n",
    "where states $s=(u,h)$ correspond to visible and hidden binary states and weights of connectivity $\\lambda=\\{a_i, b_j, w_{ij}\\}$. The assigned probability of activity configuration is given by Boltzman distribution:\n",
    "\n",
    "$P_\\lambda (s) = \\frac{1}{Z(\\beta,\\lambda)}e^{-\\beta E(s,\\lambda)}$\n",
    "\n",
    "where $Z$ is the partition function and $\\beta=1/T$ for $k_B=1$. We can easily find the conditional probabilities:\n",
    "\n",
    "$P_\\lambda(v|h_j=1) = \\sigma(\\beta b_j + \\beta \\Sigma v_i w_{ij})$\n",
    "\n",
    "$P_\\lambda(h|v_i=1) = \\sigma(\\beta a_i + \\beta \\Sigma h_j w_{ij})$\n",
    "\n",
    "where $\\sigma(x)=\\frac{1}{1+exp(-x)}$ is the sigmoid activation function. We then derive the weight update rule during learning:\n",
    "\n",
    "$\\frac{\\delta \\log P_\\lambda(v)}{\\delta w_{ij}} = <v_ih_j>_{data} - <v_ih_j>_{model}$\n",
    "\n",
    "$\\delta w_{ij} = \\epsilon (<v_ih_j>_{data} - <v_ih_j>_{model})$\n",
    "\n",
    "where $\\epsilon$ is the learning rate and brackets are the expected value corresponding to data and model. Gibbs sampling is used for $<v_ih_j>_{model}$. An alternative is \"reconstruction\" hidden activity rather than simulation and sampling. This is related to the KL-divergence or Contrast Divergence (CD) objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- Hinton's practical guide: https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf\n",
    "- Equivalence and phase space of Hopfield network: https://arxiv.org/pdf/1105.2790.pdf\n",
    "- Thermodynamics and learning dynamics: https://arxiv.org/pdf/1803.01960.pdf\n",
    "- Non-equilibrium thermodynamics of RBM: https://arxiv.org/pdf/1704.08724.pdf\n",
    "- Deep-BM for MNIST data: https://physics.bu.edu/~pankajm/ML-Notebooks/HTML/NB_CXVI_RBM_mnist.html\n",
    "- Simple verision from scratch: https://github.com/echen/restricted-boltzmann-machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.3455138  -0.19073214 -0.02561448  1.00467307]\n",
      " [ 0.61367441 -6.33991472  5.21880641  0.13222098]\n",
      " [ 0.15749267 -5.28746653  3.65076567 -7.44982278]\n",
      " [ 4.89646827  1.32896694  1.47909959  2.39573458]\n",
      " [ 0.31553433  5.4729612  -6.05912589  0.17479027]\n",
      " [ 0.18157511  3.75017398 -5.21915974 -7.51085593]\n",
      " [-3.87302123 -2.30221829 -2.26667749 -2.16470879]]\n",
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "class RBM:\n",
    "    def __init__(self, num_visible, num_hidden):\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_visible = num_visible\n",
    "        self.debug_print = False #True\n",
    "\n",
    "        # Initialize a weight matrix, of dimensions (num_visible x num_hidden), using\n",
    "        # a uniform distribution between -sqrt(6. / (num_hidden + num_visible))\n",
    "        # and sqrt(6. / (num_hidden + num_visible)). One could vary the \n",
    "        # standard deviation by multiplying the interval with appropriate value.\n",
    "        # Here we initialize the weights with mean 0 and standard deviation 0.1. \n",
    "        # Reference: Understanding the difficulty of training deep feedforward \n",
    "        # neural networks by Xavier Glorot and Yoshua Bengio\n",
    "        np_rng = np.random.RandomState(1234)\n",
    "        self.weights = np.asarray(np_rng.uniform(\n",
    "            low=-0.1 * np.sqrt(6. / (num_hidden + num_visible)),\n",
    "            high=0.1 * np.sqrt(6. / (num_hidden + num_visible)),\n",
    "            size=(num_visible, num_hidden)))\n",
    "\n",
    "        # Insert weights for the bias units into the first row and first column.\n",
    "        self.weights = np.insert(self.weights, 0, 0, axis = 0)\n",
    "        self.weights = np.insert(self.weights, 0, 0, axis = 1)\n",
    "\n",
    "    def train(self, data, max_epochs = 1000, learning_rate = 0.1):\n",
    "        \"\"\"\n",
    "        Train the machine.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: A matrix where each row is a training example consisting of the states of visible units.    \n",
    "        \"\"\"\n",
    "\n",
    "        num_examples = data.shape[0]\n",
    "\n",
    "        # Insert bias units of 1 into the first column.\n",
    "        data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "        for epoch in range(max_epochs):      \n",
    "          # Clamp to the data and sample from the hidden units. \n",
    "          # (This is the \"positive CD phase\", aka the reality phase.)\n",
    "            pos_hidden_activations = np.dot(data, self.weights)      \n",
    "            pos_hidden_probs = self._logistic(pos_hidden_activations)\n",
    "            pos_hidden_probs[:,0] = 1 # Fix the bias unit.\n",
    "            pos_hidden_states = pos_hidden_probs > np.random.rand(num_examples, self.num_hidden + 1)\n",
    "            # Note that we're using the activation *probabilities* of the hidden states, not the hidden states       \n",
    "            # themselves, when computing associations. We could also use the states; see section 3 of Hinton's \n",
    "            # \"A Practical Guide to Training Restricted Boltzmann Machines\" for more.\n",
    "            pos_associations = np.dot(data.T, pos_hidden_probs)\n",
    "\n",
    "            # Reconstruct the visible units and sample again from the hidden units.\n",
    "            # (This is the \"negative CD phase\", aka the daydreaming phase.)\n",
    "            neg_visible_activations = np.dot(pos_hidden_states, self.weights.T)\n",
    "            neg_visible_probs = self._logistic(neg_visible_activations)\n",
    "            neg_visible_probs[:,0] = 1 # Fix the bias unit.\n",
    "            neg_hidden_activations = np.dot(neg_visible_probs, self.weights)\n",
    "            neg_hidden_probs = self._logistic(neg_hidden_activations)\n",
    "            # Note, again, that we're using the activation *probabilities* when computing associations, not the states \n",
    "            # themselves.\n",
    "            neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs)\n",
    "\n",
    "            # Update weights.\n",
    "            self.weights += learning_rate * ((pos_associations - neg_associations) / num_examples)\n",
    "\n",
    "            error = np.sum((data - neg_visible_probs) ** 2)\n",
    "            if self.debug_print:\n",
    "                print(\"Epoch %s: error is %s\" % (epoch, error))\n",
    "\n",
    "    def run_visible(self, data):\n",
    "        \"\"\"\n",
    "        Assuming the RBM has been trained (so that weights for the network have been learned),\n",
    "        run the network on a set of visible units, to get a sample of the hidden units.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: A matrix where each row consists of the states of the visible units.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden_states: A matrix where each row consists of the hidden units activated from the visible\n",
    "        units in the data matrix passed in.\n",
    "        \"\"\"\n",
    "\n",
    "        num_examples = data.shape[0]\n",
    "\n",
    "        # Create a matrix, where each row is to be the hidden units (plus a bias unit)\n",
    "        # sampled from a training example.\n",
    "        hidden_states = np.ones((num_examples, self.num_hidden + 1))\n",
    "\n",
    "        # Insert bias units of 1 into the first column of data.\n",
    "        data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "        # Calculate the activations of the hidden units.\n",
    "        hidden_activations = np.dot(data, self.weights)\n",
    "        # Calculate the probabilities of turning the hidden units on.\n",
    "        hidden_probs = self._logistic(hidden_activations)\n",
    "        # Turn the hidden units on with their specified probabilities.\n",
    "        hidden_states[:,:] = hidden_probs > np.random.rand(num_examples, self.num_hidden + 1)\n",
    "        # Always fix the bias unit to 1.\n",
    "        # hidden_states[:,0] = 1\n",
    "\n",
    "        # Ignore the bias units.\n",
    "        hidden_states = hidden_states[:,1:]\n",
    "        return hidden_states\n",
    "\n",
    "    # TODO: Remove the code duplication between this method and `run_visible`?\n",
    "    def run_hidden(self, data):\n",
    "        \"\"\"\n",
    "        Assuming the RBM has been trained (so that weights for the network have been learned),\n",
    "        run the network on a set of hidden units, to get a sample of the visible units.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: A matrix where each row consists of the states of the hidden units.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        visible_states: A matrix where each row consists of the visible units activated from the hidden\n",
    "        units in the data matrix passed in.\n",
    "        \"\"\"\n",
    "\n",
    "        num_examples = data.shape[0]\n",
    "\n",
    "        # Create a matrix, where each row is to be the visible units (plus a bias unit)\n",
    "        # sampled from a training example.\n",
    "        visible_states = np.ones((num_examples, self.num_visible + 1))\n",
    "\n",
    "        # Insert bias units of 1 into the first column of data.\n",
    "        data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "        # Calculate the activations of the visible units.\n",
    "        visible_activations = np.dot(data, self.weights.T)\n",
    "        # Calculate the probabilities of turning the visible units on.\n",
    "        visible_probs = self._logistic(visible_activations)\n",
    "        # Turn the visible units on with their specified probabilities.\n",
    "        visible_states[:,:] = visible_probs > np.random.rand(num_examples, self.num_visible + 1)\n",
    "        # Always fix the bias unit to 1.\n",
    "        # visible_states[:,0] = 1\n",
    "\n",
    "        # Ignore the bias units.\n",
    "        visible_states = visible_states[:,1:]\n",
    "        return visible_states\n",
    "\n",
    "    def daydream(self, num_samples):\n",
    "        \"\"\"\n",
    "        Randomly initialize the visible units once, and start running alternating Gibbs sampling steps\n",
    "        (where each step consists of updating all the hidden units, and then updating all of the visible units),\n",
    "        taking a sample of the visible units at each step.\n",
    "        Note that we only initialize the network *once*, so these samples are correlated.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        samples: A matrix, where each row is a sample of the visible units produced while the network was\n",
    "        daydreaming.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a matrix, where each row is to be a sample of of the visible units \n",
    "        # (with an extra bias unit), initialized to all ones.\n",
    "        samples = np.ones((num_samples, self.num_visible + 1))\n",
    "\n",
    "        # Take the first sample from a uniform distribution.\n",
    "        samples[0,1:] = np.random.rand(self.num_visible)\n",
    "\n",
    "        # Start the alternating Gibbs sampling.\n",
    "        # Note that we keep the hidden units binary states, but leave the\n",
    "        # visible units as real probabilities. See section 3 of Hinton's\n",
    "        # \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
    "        # for more on why.\n",
    "        for i in range(1, num_samples):\n",
    "            visible = samples[i-1,:]\n",
    "\n",
    "            # Calculate the activations of the hidden units.\n",
    "            hidden_activations = np.dot(visible, self.weights)      \n",
    "            # Calculate the probabilities of turning the hidden units on.\n",
    "            hidden_probs = self._logistic(hidden_activations)\n",
    "            # Turn the hidden units on with their specified probabilities.\n",
    "            hidden_states = hidden_probs > np.random.rand(self.num_hidden + 1)\n",
    "            # Always fix the bias unit to 1.\n",
    "            hidden_states[0] = 1\n",
    "\n",
    "            # Recalculate the probabilities that the visible units are on.\n",
    "            visible_activations = np.dot(hidden_states, self.weights.T)\n",
    "            visible_probs = self._logistic(visible_activations)\n",
    "            visible_states = visible_probs > np.random.rand(self.num_visible + 1)\n",
    "            samples[i,:] = visible_states\n",
    "\n",
    "        # Ignore the bias units (the first column), since they're always set to 1.\n",
    "        return samples[:,1:]        \n",
    "    \n",
    "    def _logistic(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    r = RBM(num_visible = 6, num_hidden = 3)\n",
    "    training_data = np.array([[1,1,1,0,0,0],[1,0,1,0,0,0],[1,1,1,0,0,0],[0,0,1,1,1,0], [0,0,1,1,0,0],[0,0,1,1,1,0]])\n",
    "    r.train(training_data, max_epochs = 5000)\n",
    "    print(r.weights)\n",
    "    user = np.array([[0,0,0,1,1,0]])\n",
    "    print(r.run_visible(user))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93660736, 0.0967272 , 0.36854321, 0.42955921, 0.61371294,\n",
       "        0.90350855],\n",
       "       [1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given a new set of visible units, we can see what hidden units are activated.\n",
    "#visible_data = np.array([[0,0,0,1,1,0]]) # A matrix with a single row that contains the states of the visible units. (We can also include more rows.)\n",
    "#r.run_visible(visible_data) # See what hidden units are activated.\n",
    "\n",
    "# Given a set of hidden units, we can see what visible units are activated.\n",
    "hidden_data = np.array([[1,0,0]]) # A matrix with a single row that contains the states of the hidden units. (We can also include more rows.)\n",
    "r.run_hidden(hidden_data) # See what visible units are activated.\n",
    "\n",
    "# We can let the network run freely (aka, daydream).\n",
    "r.daydream(10) # Daydream for 100 steps on a single initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Energy(v,h,W):\n",
    "    return np.dot(np.dot(v,W),h.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Macintosh/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:42: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "### Training RBM ###\n",
    "data_input = np.array([[1,1,1,0,0,0],[1,0,1,0,0,0],[1,1,1,0,0,0],[0,0,1,1,1,0],[0,0,1,1,0,0],[0,0,1,1,1,0]]) #exampleXfeature\n",
    "data_input = data_input[:-2,:].copy()\n",
    "max_ep = 1000\n",
    "learn_rate = 0.1\n",
    "num_exp = data_input.shape[0]  \n",
    "num_vis = data_input.shape[1]  #corresponding to visible input\n",
    "num_hid = 4  #number of hidden units\n",
    "W = np.random.rand(num_vis,num_hid)#(num_exp, num_hid)\n",
    "W = np.insert(W, 0, 0, axis = 0)\n",
    "W = np.insert(W, 0, 0, axis = 1)\n",
    "data = np.insert(data_input, 0, 1, axis = 1)  #bias term in the first column\n",
    "Ens = []\n",
    "Ers = []\n",
    "E11s = []\n",
    "E01s = []\n",
    "\n",
    "for ep in range(max_ep):\n",
    "    #Clamp (postive CD)\n",
    "    P_h_act = np.dot(data, W)\n",
    "    P_h = logistic(P_h_act)\n",
    "    P_h[:,0] = 1  #fix bias\n",
    "    P_h_state = P_h > np.random.rand(num_exp, num_hid + 1)\n",
    "    P_aso = np.dot(data.T, P_h)\n",
    "\n",
    "    #Reconstruct (negative CD)\n",
    "    n_v_act = np.dot(P_h_state, W.T)\n",
    "    n_v = logistic(n_v_act)\n",
    "    n_v[:,0] = 1\n",
    "    n_v_state = n_v > np.random.rand(num_exp, num_vis + 1) ##\n",
    "    n_h_act = np.dot(n_v, W)\n",
    "    n_h = logistic(n_h_act)\n",
    "    n_h_state = n_h > np.random.rand(num_exp, num_hid + 1) ##\n",
    "    n_aso = np.dot(n_v.T, n_h)\n",
    "    \n",
    "    E01 = np.sum(Energy(n_v_state, n_h_state, W)) ##\n",
    "    #Update weights\n",
    "    W += learn_rate* ((P_aso - n_aso) / num_exp)\n",
    "    E11 = np.sum(Energy(n_v_state, n_h_state, W)) ##\n",
    "    \n",
    "    error = np.sum((data - n_v) ** 2)  #error term\n",
    "    Ens.append(np.nansum(np.log(P_h_act+10**-8)))\n",
    "    Ers.append(error)\n",
    "    E11s.append(E11)\n",
    "    E01s.append(E01)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "E11s = np.array(E11s)\n",
    "E01s = np.array(E01s)\n",
    "\n",
    "Qs = E11s[1:]-E01s[1:]\n",
    "Ws = E01s[1:]-E11s[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x110d6ca20>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD7CAYAAACIYvgKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e7a87b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(Ens))\n",
    "plt.hold(True)\n",
    "plt.plot(np.array(Ers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Re-activate\n",
    "num_samp = 10\n",
    "samples = np.ones((num_samples, self.num_visible + 1))\n",
    "\n",
    "        # Take the first sample from a uniform distribution.\n",
    "        samples[0,1:] = np.random.rand(self.num_visible)\n",
    "\n",
    "        # Start the alternating Gibbs sampling.\n",
    "        # Note that we keep the hidden units binary states, but leave the\n",
    "        # visible units as real probabilities. See section 3 of Hinton's\n",
    "        # \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
    "        # for more on why.\n",
    "        for i in range(1, num_samples):\n",
    "            visible = samples[i-1,:]\n",
    "\n",
    "            # Calculate the activations of the hidden units.\n",
    "            hidden_activations = np.dot(visible, self.weights)      \n",
    "            # Calculate the probabilities of turning the hidden units on.\n",
    "            hidden_probs = self._logistic(hidden_activations)\n",
    "            # Turn the hidden units on with their specified probabilities.\n",
    "            hidden_states = hidden_probs > np.random.rand(self.num_hidden + 1)\n",
    "            # Always fix the bias unit to 1.\n",
    "            hidden_states[0] = 1\n",
    "\n",
    "            # Recalculate the probabilities that the visible units are on.\n",
    "            visible_activations = np.dot(hidden_states, self.weights.T)\n",
    "            visible_probs = self._logistic(visible_activations)\n",
    "            visible_states = visible_probs > np.random.rand(self.num_visible + 1)\n",
    "            samples[i,:] = visible_states\n",
    "\n",
    "        # Ignore the bias units (the first column), since they're always set to 1.\n",
    "        return samples[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
